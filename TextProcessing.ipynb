{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextProcessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_8TNqoEGAJQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "cd0c6b7e-ed46-43f4-b4d7-77f75690f1cd"
      },
      "source": [
        "# Libraries\n",
        "\n",
        "import keras"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoGbYb52Goqw",
        "colab_type": "text"
      },
      "source": [
        "Load data stored online"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiKfwEFLGQOX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data loading method\n",
        "\n",
        "def load_data_from_url(url, txt_file_name=\"text_file.txt\"):\n",
        "  path_to_file = keras.utils.get_file(txt_file_name, url)\n",
        "  # Read, then decode for py2 compat.\n",
        "  text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "val3EFzqGu0W",
        "colab_type": "text"
      },
      "source": [
        "Perform pre-processing operatiorns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3n87JQYGHJG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Processing method\n",
        "\n",
        "def process_text(text, min_word_frequency=10, seq_len=10):\n",
        "  '''Performs preprocessing step for a given text corpus'''\n",
        "  clean_text = text.replace('\\n', ' \\n ')\n",
        "  # Split with new lines consideration\n",
        "  clean_text = [w for w in clean_text.split(' ') if w.strip() != '' or w == '\\n']\n",
        "\n",
        "  # Work upon word freqs.\n",
        "  word_freq = {}\n",
        "  for word in clean_text:\n",
        "    word_freq[word] = word_freq.get(word, 0) + 1\n",
        "    \n",
        "  # Check for min frequency condition\n",
        "\n",
        "  ignored_words = set()\n",
        "  for k, v in word_freq.items():\n",
        "    if word_freq[k] < min_word_frequency:\n",
        "      ignored_words.add(k)\n",
        "\n",
        "  words = set(clean_text)\n",
        "  print('Unique words before ignoring:', len(words))\n",
        "  print('Ignoring words with frequency <', min_word_frequency)\n",
        "  words = sorted(set(words) - ignored_words)\n",
        "  print('Unique words after ignoring:', len(words))\n",
        "  # cut the text in semi-redundant sequences of SEQUENCE_LEN words\n",
        "  STEP = 1\n",
        "  sentences = []\n",
        "  next_words = []\n",
        "  ignored = 0\n",
        "  for i in range(0, len(clean_text) - seq_len, STEP):\n",
        "      # Only add sequences where no word is in ignored_words\n",
        "      if len(set(clean_text[i: i+seq_len+1]).intersection(ignored_words)) == 0:\n",
        "          sentences.append(clean_text[i: i + seq_len])\n",
        "          next_words.append(clean_text[i + seq_len])\n",
        "      else:\n",
        "          ignored = ignored+1\n",
        "  print('Ignored sequences:', ignored)\n",
        "  print('Remaining sequences:', len(sentences))\n",
        "\n",
        "  return (word_freq, words, ignored_words, sentences, next_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFk721x4Ia4h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def shuffle_and_split_training_set(sentences_original, next_original, percentage_test=2):\n",
        "    '''Performs split of dataset'''\n",
        "    # shuffle at unison\n",
        "    print('Shuffling sentences')\n",
        "\n",
        "    tmp_sentences = []\n",
        "    tmp_next_word = []\n",
        "    for i in np.random.permutation(len(sentences_original)):\n",
        "        tmp_sentences.append(sentences_original[i])\n",
        "        tmp_next_word.append(next_original[i])\n",
        "\n",
        "    cut_index = int(len(sentences_original) * (1.-(percentage_test/100.)))\n",
        "    x_train, x_test = tmp_sentences[:cut_index], tmp_sentences[cut_index:]\n",
        "    y_train, y_test = tmp_next_word[:cut_index], tmp_next_word[cut_index:]\n",
        "\n",
        "    print(\"Size of training set = %d\" % len(x_train))\n",
        "    print(\"Size of test set = %d\" % len(y_test))\n",
        "    return (x_train, y_train), (x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDnYJig2LyZr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_url = 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXWs9bebL3ya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = load_data_from_url(data_url)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5fP2LsUMQ0w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "c5e9bf7c-1293-4b12-8f3f-d6649d4f8f5b"
      },
      "source": [
        "(word_freq, words, ignored_words, sentences, next_words) = process_text(result)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique words before ignoring: 25671\n",
            "Ignoring words with frequency < 10\n",
            "Unique words after ignoring: 2154\n",
            "Ignored sequences: 219742\n",
            "Remaining sequences: 22899\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}